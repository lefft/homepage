

~~~~~~~~~~
Consider the following scenario, which I imagine will be familiar to many academics: you hear about a new study that's supposed to have some results relevant to some aspect of your research program. So you find the paper online, download it as a pdf, and then start reading it. Your goal as a critical reader is to identify what the authors' main claim is, to understand what empirical evidence they provide for that claim, and to evaluate the logic that deductively flows from a set of competing hypotheses to the conclusion that one of these hypotheses is supported by the main empirical result(s) of the paper.

In a perfect world, scientists' rhetorical writing strategies should not affect how likely a reader is to accept their conclusions. 

But alas: our world is filled with all kinds of biases and preconceptions and 
~~~~~~~~~~




~~~~~~~~~~
Abstracting (and perhaps idealizing) a bit, scientific papers can -- arguably *should* -- be viewed as nothing more than logical syllogisms, written in a prosaic style. In the world of this "analogy," introduction and conclusion sections serve basically as context, with the purpose of helping the reader follow the authors' reasoning. Viewed this way, the scientific merits of a study should be objectively measurable: does the authors' conclusion ("main claim") logically follow from their theoretical assumptions and the results of their experiment(s)? Is the result clear enough that we can reject the notion that it arose from chance? 
~~~~~~~~~~







~~~~~~~~~~
Maybe you start by reading the abstract, then the results section, then the conclusion. Then you might revisit the results section in more detail, perhaps visually dissecting whatever plots or tables are provided. Once you've digested enough of the material, you begin to make your evaluation of the paper's scientific merits. This is a common strategy, maybe the most common. As graduate students we might even be taught that something like this is the best way to read a journal article with an experimental component. For quick reference we'll call it the *skim-and-iterate* method. 

Another strategy -- probably the default strategy for reading things in general -- is to skim the abstract, then just start reading at the first word of the first page, and stop reading after the last word of the last page (possibly skimming or skipping immaterial or opaque parts). If the paper is written according to the standard intro-methods-procedure-results-discussion-conclusion format, then you'll be in a position to evaluate the authors' main claim as soon as you get through the results/discussion parts. Assuming you're still awake, of course. Let's call this the *just-read-it* strategy. 
~~~~~~~~~~








#### sketch of study evaluating 'onion method'

##### design

~~~~~~~~~~~
2x2 factorial design:

	format (just-read-it vs onion) x rhetoric (overstated/conservative)

both factors between subj

run power analysis for sample size beforehand
~~~~~~~~~~~

##### materials 

~~~~~~~~~~~
main components of toy paper (draft below):

	claim:  computers can convince ppl that they're ppl
	expt:   100 ppl chat w a chatbot or person, have to decide if chatter is a person or bot
	result: xx% of ppl in bot condition said person; yy% of ppl in person cond said person

sections of paper: 

	sec1: introduction 		 (varies across levels of rhetoric)
	sec2: description of study 	 (identical across levels of rhetoric)
	sec3: results 			 (identical across levels of rhetoric)
	sec4: conclusion 		 (varies across levels of rhetoric)

presentation order: 

	just-read-it: sec1, sec2, sec3, sec4
	onion:        sec2, sec3, sec1, sec4

comprehension questions, asked at each section:

	comp1: what was the author's main conclusion? [choices]
	comp2: the author measured whether ppl [enjoyed chatting w a person or bot better]
	comp3: the author found that [most ppl could distinguish btwn bot + person]
	comp4: the author's hypothesis was that [ppl cannot distinguish btwn bot + person]

critical evaluation questions, asked at end: 

	crit1: did the experiment prove the author's claim to be true?
	crit2: did the experiment provide evidence for the author's claim?
	crit3: do you believe the author's main claim is true?
	crit4: if this was real, would yr opinion abt author's claim change after reading?
~~~~~~~~~~~


#### drafts of materials to be used

##### toy paper (overstated condition)

- **introduction (overstated condition):** 
in the modern world, computers are becoming more human-like. however, until now it has not been known whether people are actually convinced by the human-like behavior of computerized agents built with cutting-edge technology (for example "chat bots"). in this paper, we [phr1: prove that] humans [phr2: cannot] tell the difference between interacting with a modern computerized "chat bot" and a real, flesh-and-blood person. in fact, [phr3: generally speaking,] people [phr4: are] *more* likely to believe that chat bots are human than they are to believe that other humans are human [phr5: ]. our central claim is that interacting with a human and interacting with a computer is largely the same kind of experience.

- **introduction (conservative condition):** 
in the modern world, computers are becoming more human-like. however, until now it has not been known whether people are actually convinced by the human-like behavior of computerized agents built with cutting-edge technology (for example "chat bots"). in this paper, we [phr1: show that under certain conditions, some] humans [phr2: may not be able to] tell the difference between interacting with a modern computerized "chat bot" and a real flesh-and-blood person. in fact, [phr3: sometimes] people [phr4: can be] *more* likely to believe that a chat bot is human than they are to believe that an actual human is human [phr5: (at least when they cannot hear the voice of the person/bot they are chatting with)]. our central claim is that interacting with a human and interacting with a computer is largely the same kind of experience.


- **methods/materials:** 
over the course of two weeks, 200 people came to the lab to participate in the experiment. each person was randomly assigned to one of two groups, but no person was aware of which group they had been assigned to. the two groups were the *chat-with-human* group, and the *chat-with-bot* group. participants in the chat-with-human group spent 10 minutes messaging back and forth on a laptop with a person, who was not visible to the participant. participants in the chat-with-bot group spent 10 minutes messaging back-and-forth with a "chat bot" -- a computer program designed to interact with ordinary people as if it were another ordinary person. after the 10-minute chat session, participants had to say whether they believed they were chatting with a real person, or with a chat-bot. 


- **results:** 
among the people who chatted with a real person (the *chat-with-human* group), 60% said they believed that they were chatting with a real person. among the people who chatted with a computer, 70% said they believed that they were chatting with a real person.


- **discussion/conclusion (overstated condition):** 
our experiment [c1: showed definitively] that modern computer technology is so good at mimicking human behavior that [c2: can easily fool people into thinking it is human]. [c3: specifically], people are more likely to believe that a chat bot is human than they are to believe that an actual person is human. this study showed that [c4: ] a computerized bot's interactions are judged to be more humanlike than an actual human's interactions. this result [c5: provides clear evidence that] robots [will soon] become ordinary members of society, just like real, flesh-and-blood humans.

- **discussion/conclusion (conservative condition):** 
our experiment [c1: provided evidence] that modern computer technology is so good at mimicking human behavior that [c2: sometimes people are not able to figure out whether they are chatting with a human or a bot]. [c3: under certain conditions,] people are more likely to believe that a chat bot is human than they are to believe that an actual person is human. this study showed that [c4: when people are not certain whether they are chatting with a real person, ] a computerized bot's interactions are judged to be more humanlike than an actual human's interactions. this result [c5: raises the question whether] robots [could soon] become ordinary members of society, just like real, flesh-and-blood humans. 



#### extra notes/thoughts

dont know if ppl have studied this before -- deliberitaly not looking into it until study done

have ppl answer comp q's before moving on(?)
(all at the end cd be bad -- ppl just flip thru; altho cd manipulate this too...)

intuition/summary of proj: write a v short expt'al paper, display a paragraph at a time linearly to one group of turkers; display the same paper in the "onion" style to another group, ask them both comprehension q's and q's about did they find it convincing. hypothesis: for convincing papers, no diff; for unconvincing papers, onion subj's believe conclusions less. BUT hopefully would be no diff in level of understanding, regardless of whether paper is convincing.

"separating argumentation from evidence"

i realize the irony that i'm explaining the motivation before presenting the study, but oh well. 

shd all q's be multiple choice? (prob bc better for analysis)