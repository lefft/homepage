---
author: "timothy leffel  &nbsp;&nbsp; may30/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE)
```

<hr style="height:2px; background-color: gray; color:gray;">

The use of twitter data for estimating public sentiment about issues can still be considered a fledgling-ish field. Here we assess the degree to which a fairly naive sentiment classifier can be used to predict the trajectory of the average bitcoin trading price, over time. The catch is that we'll only use data publicly available via the twitter API. This is limiting in a few respects, chief among which is that targeting specific historical date ranges is basically impossible with only the standard twitter API tools (to my knowledge, at least). 


<br>

### 1. btc prices from twitter

<hr style="height:1px; background-color:lightgray; color:lightgray;">

#### set up twitter api

```{r include=FALSE}
# load dependencies -- for data acquisition and processing
library("twitteR");   library("ROAuth");   library("httr")
library("lubridate"); library("reshape2"); library("dplyr")

# load dependencies -- for plotting results
library("ggplot2"); library("scales")

# set auth cache to false to avoid prompts in `setup_twitter_oauth()`
options(httr_oauth_cache=FALSE)
```


```{r}
# read in api keys and access tokens
keyz <- read.csv("../../../../keyz.csv", header=TRUE, stringsAsFactors=FALSE)
# set up twitter auth -- call `setup_twitter_oauth()` w keys from `keyz`
setup_twitter_oauth(
  consumer_key    = keyz$value[keyz$auth_type=="twtr_api_key"], 
  consumer_secret = keyz$value[keyz$auth_type=="twtr_api_sec"], 
  access_token    = keyz$value[keyz$auth_type=="twtr_acc_tok"], 
  access_secret   = keyz$value[keyz$auth_type=="twtr_acc_sec"]
)

```


#### get btc price tweets + clean them up

```{r}
# get tweets from user @bitcoinprice for maximum n (3200 as of may2017)

# @bitcoinprice
# The average price of Bitcoin across all exchanges is ... 
dat <- userTimeline(user="bitcoinprice", n=3200, excludeReplies=TRUE) %>% 
  twListToDF() %>% 
  select(text, screenName, created, favoriteCount, retweetCount, isRetweet) %>% 
  mutate(date=date(created), hour=hour(created))

# a tweet consists of "toss[1] + dddd.dd + toss[2]"
toss <- c("The average price of Bitcoin across all exchanges is ", " USD")

# check that all tweets are formulaic + as expected
if (!all(grepl(paste0(toss[1], "\\d*\\.?\\d*", toss[2]), dat$text))){
  message("careful -- tweets not as formulaic as they may seem! :/ ")
}

# if tweets are formulaic, just remove `toss` + extract price from `dat$tweet`
dat$price <- gsub(paste0(toss, collapse="|"), "", dat$text)

# check that we don't have any words/letters left before converting
if (sum(grepl("[a-z]", dat$price)) > 0){
  warning("careful -- some tweets were not in expected format! :o ")
}

# now convert to numeric
dat$price <- as.numeric(dat$price)

# delete any retweets that could've crept in
dat <- dat[!dat$isRetweet, ]

# convert date_time to nicer format [CHECK THAT MOVING FROM lt TO CT IS OKAY]
dat$date_time <- as.POSIXct(dat$created, tz="UTC")

# get day of the week so we can look at weekly trends
dat$day <- weekdays(dat$date_time, abbreviate=TRUE)
dat$day <- factor(dat$day, levels=c("Mon","Tue","Wed","Thu","Fri","Sat","Sun"))
# "Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"

# notice that on 2017-03-14 15:00:03, the value is listed at $1.25
# inspecting the surrounding data suggests it's probably meant to be $1225.00.
# **exercise**: what does this suggest about how the bot works?
dat$price[dat$price==1.25] <- dat$price[dat$price==1.25] * 1000

# now cut the df to just the cols we want
dat <- dat[, c("screenName","created","date","hour","day","price")]

```


#### plot the data

```{r}
# what's our date range? (3200 hourly tweets so should be 3200/24 days ~ 4mo)
# from "2017-01-15" to "2017-05-29"
# range(dat$date)

# now make some plots!
ggplot(dat, aes(x=created, y=price)) +
  geom_line() + scale_y_continuous(limits=c(0, 3000))

pdat <- dat %>% select(day, hour, price) %>% group_by(day, hour) %>% summarize(
  mean_price = mean(price)
) %>% data.frame()

ggplot(pdat, aes(x=hour, y=mean_price)) + 
  geom_line() + 
  facet_wrap(~day, ncol=7)

```


#### inspect + validate accuracy on external source

```{r}
### get external data now
bdat <- read.csv(
  "data/bitcoinity_data-price-volume.csv", 
  col.names=c("date","price","volume"),
  colClasses=c("Date","numeric","numeric")
)
bdat$screenName <- "bitcoinity.org"

# the twitter data:
ggplot(dat, aes(x=created, y=price)) +
  geom_line() + scale_y_continuous(limits=c(0, 3000))

# the external data, plotted on the same interval:
ggplot(bdat[bdat$date >= min(dat$date), ], aes(x=date, y=price)) +
  geom_line() + scale_y_continuous(limits=c(0, 3000))

# the external data, plotted on the whole lifetime of btc:
ggplot(bdat[bdat$date >= "2017-01-01", ], aes(x=date, y=price)) +
  geom_line() + scale_y_continuous(limits=c(0, 3000)) +
  geom_line(aes(x=date, y=volume/1e6), color="orange") +
  scale_x_date(date_breaks="2 weeks") +
  geom_hline(yintercept=max(bdat$price), color="#8aa8b5", linetype="dashed") +
  annotate(geom="text", x=as.Date("2017-02-01"), y=max(bdat$price)+100, 
           color="#8aa8b5", label="all-time high")


# want to merge twitter data with bitcoinity data
# first need to aggregate twitter data to the day-level
tdat_simple <- dat %>% 
  mutate(date=as.character(date)) %>% 
  select(screenName, date, price) %>% 
  group_by(screenName, date) %>% summarize(
  price = mean(price, na.rm=TRUE)
) %>% data.frame()

bdat_simple <- bdat %>% 
  filter(date >= min(dat$date)) %>% 
  select(screenName, date, price) %>% 
  mutate(date=as.character(date))

head(tdat_simple, 2); head(bdat_simple, 2)

merged <- rbind(tdat_simple, bdat_simple)
merged$date <- as.Date(merged$date)

ggplot(merged, aes(x=date, y=price, group=screenName, color=screenName)) +
  geom_line() + 
  scale_y_continuous(limits=c(0, 3000)) +
  scale_x_date(date_breaks="2 weeks") +
  theme(axis.text.x=element_text(angle=45, vjust=1, hjust=1))

```


### 2. tweet sentiment about btc

<hr style="height:1px; background-color:lightgray; color:lightgray;">

#### enter classifier

```{r}
# https://github.com/ropensci/monkeylearn
library("monkeylearn")

keys <- read.csv("../../../../keyz.csv", stringsAsFactors=FALSE)
ml_key  <- keys$value[keys$auth_type=="monkey_pkey"]

```


```{r}
# the confusion matrix of the classifier at may30 4pm
# Predicted
# ne	ne	po
# negative	3,361	1,017	504
# neutral	702	6,770	1,315
# positive	486	1,925	4,852

```


```{r}
boosh <- function(text){
  monkeylearn_classify(
    request=text, 
    key=ml_key, 
    classifier_id="cl_qkjxv9Ly",
    verbose=TRUE
  )
}
```


#### "validate" classifier on unabomber manifesto [sloppy!]

```{r eval=FALSE}

# going to use this classifier:
#   https://app.monkeylearn.com/main/classifiers/cl_qkjxv9Ly/
# manif <- readLines("input/unabomber-manifesto.txt")
# blaowwie <- boosh(manif)
```


#### average sentiment measure abt btc (last ~4mo)


```{r}
# get random sample of btc tweets from last 4mo,
# then get sentiment score

### NEED TO LOOP OVER DAYS
dates <- 
  as.Date(sample(1:100, size=10), origin="2017-02-01") %>% 
  as.character()

get_tweets <- function(date){
  tweets <- searchTwitter("bitcoin", n=100, resultType="popular", since=date)
  return(tweets)
}

waow <- twListToDF(sapply(dates, get_tweets))

waow_clf <- boosh(text=waow$text)

# may want to remove some
plot(table(waow_clf$label))
hist(waow_clf$probability)

```


### 3. find turbulent btc days

<hr style="height:1px; background-color:lightgray; color:lightgray;">

#### identify sharp increase day

```{r}
# A question that Iâ€™m often asked is how to retrieve data from the past, generally
# people are doing a study on some major event that has already happened (e.g.
# Arab Spring, an election, etc). Using the Twitter API this is impossible as
# you can only go back a small amount. However, if you have the ability to
# look ahead, it is easy to enable a prospective study by collecting data and
# automatically persisting it to a database. Th

# identify sharp increase day via tweet data
```


#### identify sharp decrease day

```{r}
# identify sharp decrease day via tweet data
```



### 4. analyze btc-related tweets on turbulent btc days

<hr style="height:1px; background-color:lightgray; color:lightgray;">

#### measure tweet sentiment on those days

```{r}
# btc sentiment on inc_day
```


```{r}
# btc sentiment on dec_day
```


#### discuss



### 5. next steps

<hr style="height:1px; background-color:lightgray; color:lightgray;">

#### direction of causation unclear

#### finer-grained temporal resolution

#### how good is the classifier?


<br><br>
<hr><hr>
<br><br>




<link rel="stylesheet" type="text/css"
href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,400i,700">

<link href="https://fonts.googleapis.com/css?family=Roboto+Mono:300,400,500" rel="stylesheet">

  <style>
body {
  padding: 10px;
  font-size: 12pt;
  font-family: 'Open Sans', sans-serif;
}

h1 { 
  font-size: 20px;
  color: DarkGreen;
  font-weight: bold;
}

h2 { 
    font-size: 16px;
    color: green;
}

h3 { 
  font-size: 24px;
  color: green;
  font-weight: bold;
}

h4 { 
  font-size: 18px;
  color: green;
  font-weight: bolder;
  padding-top: 10px;
}

li {
  padding: 3px;
}

code {
  font-family: 'Roboto Mono', monospace;
  font-size: 14px;
}

pre {
  font-family: 'Roboto Mono', monospace;
  font-size: 14px;
}

p {
  margin-top: 30px;
  margin-bottom: 15px;
}

</style>



<!-- END OF DOCUMENT IS HERE -->

